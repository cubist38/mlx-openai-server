{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe5da27",
   "metadata": {},
   "source": [
    "# PDF Question Answering with Embedding Search\n",
    "\n",
    "This notebook demonstrates how to build a simple RAG (Retrieval-Augmented Generation) system that:\n",
    "\n",
    "1. Extracts and chunks text from PDF documents.\n",
    "2. Embeds the text using a local embedding model served via MLX Server.\n",
    "3. Stores the embeddings in a FAISS index for fast retrieval.\n",
    "4. Answers user queries by retrieving relevant chunks and using a chat model to respond based on context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc490ab0",
   "metadata": {},
   "source": [
    "Before running the notebook, make sure to launch the local MLX Server by executing the following command in your terminal (`lm`: Text-only model): \n",
    "```bash\n",
    "mlx-server launch --model-path mlx-community/Qwen3-4B-8bit --model-type lm\n",
    "```\n",
    "This command starts the MLX API server locally at http://localhost:8000/v1, which exposes an OpenAI-compatible interface. It enables the specified model to be used for both embedding (vector representation of text) and response generation (chat completion).\n",
    "\n",
    "For this illustration, we use the model `mlx-community/Qwen3-4B-8bit`, a lightweight and efficient language model that supports both tasks. You can substitute this with any other compatible model depending on your use case and hardware capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036359c3",
   "metadata": {},
   "source": [
    "## Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d641abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -Uq numpy PyMuPDF faiss-cpu openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a6f78",
   "metadata": {},
   "source": [
    "## Initialize MLX Server client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546b6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the local MLX Server that serves embedding and chat models\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",  # This is your MLX Server endpoint\n",
    "    api_key=\"fake-api-key\"                # Dummy key, not used by local server\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573141f",
   "metadata": {},
   "source": [
    "## Load and chunk PDF document\n",
    "\n",
    "We load the PDF file and split it into smaller chunks to ensure each chunk fits within the context window of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001354f",
   "metadata": {},
   "source": [
    "### Read PDF\n",
    "Extracts text from each page of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27be75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def read_pdf(path):\n",
    "    \"\"\"Extract text from each page of a PDF file.\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    texts = []\n",
    "    for page in doc:\n",
    "        text = page.get_text().strip()\n",
    "        if text: \n",
    "            texts.append(text)\n",
    "    doc.close()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae0912",
   "metadata": {},
   "source": [
    "### Chunk Text\n",
    "Splits the extracted text into smaller chunks with overlap to preserve context.\n",
    "We use the following parameters:\n",
    "- `chunk_size=500`: Maximum number of words in a chunk.\n",
    "- `overlap=100`: Number of words overlapping between consecutive chunks to avoid breaking context too harshly.\n",
    "\n",
    "Each chunk is created using simple whitespace (`\" \"`) tokenization and rejoined with spaces, which works well for general text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e093a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(texts, chunk_size=400, overlap=200):\n",
    "    \"\"\"Split text into smaller chunks with overlap for better context preservation.\"\"\"\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        words = text.split()  \n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            chunk = words[i:i + chunk_size]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            i += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c9e71",
   "metadata": {},
   "source": [
    "## Save embeddings and chunks to FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d76e7d",
   "metadata": {},
   "source": [
    "### Embed Chunks\n",
    "\n",
    "Uses the MLX Server to generate embeddings for the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb628d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Embed chunks using the model served by MLX Server\n",
    "def embed_chunks(chunks, model_name):\n",
    "    \"\"\"Generate embeddings for text chunks using the MLX Server model.\"\"\"\n",
    "    response = client.embeddings.create(input=chunks, model=model_name)\n",
    "    embeddings = [np.array(item.embedding).astype('float32') for item in response.data]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294297ae",
   "metadata": {},
   "source": [
    "### Save to FAISS\n",
    "\n",
    "Saves the embeddings in a FAISS index and the chunks in a metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d39501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def normalize(vectors):\n",
    "    \"\"\"\n",
    "    Normalize a set of vectors.\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)  # Compute L2 norms for each vector\n",
    "    return vectors / norms  # Divide each vector by its norm to normalize\n",
    "\n",
    "def save_faiss_index(embeddings, chunks, index_path=\"db/index.faiss\", meta_path=\"db/meta.pkl\"):\n",
    "    \"\"\"\n",
    "    The embeddings are stored in a FAISS index, \n",
    "    and the corresponding text chunks are saved in a metadata file locally.    \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"db\"):\n",
    "        os.makedirs(\"db\")  \n",
    "    dim = len(embeddings[0])\n",
    "    \n",
    "    # Normalize the embeddings to unit length for cosine similarity\n",
    "    # This is required because FAISS's IndexFlatIP uses inner product\n",
    "    embeddings = normalize(embeddings)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(meta_path, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34ab09",
   "metadata": {},
   "source": [
    "Combines the above steps into a single pipeline to process a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2a4f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: Read PDF → Chunk → Embed → Save\n",
    "def prepare_pdf(pdf_path, model_name):\n",
    "    texts = read_pdf(pdf_path)\n",
    "    chunks = chunk_text(texts)\n",
    "    embeddings = embed_chunks(chunks, model_name)\n",
    "    save_faiss_index(embeddings, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012725f5",
   "metadata": {},
   "source": [
    "## Query PDF using FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056aa04",
   "metadata": {},
   "source": [
    "### Load FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4022de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index(index_path=\"db/index.faiss\", meta_path=\"db/meta.pkl\"):\n",
    "    \"\"\"Load the FAISS index and corresponding text chunks from disk.\"\"\"\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142389ab",
   "metadata": {},
   "source": [
    "### Embed Query\n",
    "Embeds the user's query using the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4791b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query, model_name):\n",
    "    \"\"\"Convert a query string into an embedding vector.\"\"\"\n",
    "    embedding = client.embeddings.create(input=[query], model=model_name).data[0].embedding\n",
    "    return np.array(embedding).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada96dd2",
   "metadata": {},
   "source": [
    "### Retrieve Chunks\n",
    "Retrieves the top-k most relevant chunks based on the query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "267bcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, index, chunks, model_name, top_k=5):\n",
    "    \"\"\"Retrieve the top-k most relevant chunks from the FAISS index.\"\"\"\n",
    "    query_vector = embed_query(query, model_name).reshape(1, -1)\n",
    "    query_vector = normalize(query_vector)  # Normalize the query vector\n",
    "    distances, indices = index.search(query_vector, top_k)  # Search for nearest neighbors\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3491a5",
   "metadata": {},
   "source": [
    "### Generate Answer with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34a811b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(query, retrieved_chunks, model_name):\n",
    "    \"\"\"Generate a response to the query using retrieved chunks as context.\"\"\"\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the context below to answer the question.\n",
    "    Context:\n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487af341",
   "metadata": {},
   "source": [
    "Combines the query steps into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f09e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: Query → Embed → Retrieve → Answer\n",
    "def query_pdf(query, model_name=\"mlx-community/Qwen3-4B-8bit\"):\n",
    "    index, chunks = load_faiss_index()\n",
    "    top_chunks = retrieve_chunks(query, index, chunks, model_name)\n",
    "    return answer_with_context(query, top_chunks, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c53682",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b022805",
   "metadata": {},
   "source": [
    "Index text chunks from PDF into FAISS using Qwen3-4B-8bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18552003",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_pdf(\"./pdfs/lab03.pdf\", \"mlx-community/Qwen3-4B-8bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fcab9",
   "metadata": {},
   "source": [
    "Sample query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c333397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What submissions do I need to submit in this lab?\n",
      "Response:  For this lab, you need to submit the following:\n",
      "\n",
      "1. **StudentID1_StudentID2_Report.pdf**:  \n",
      "   - A short report explaining your solution, any problems encountered, and any approaches you tried that didn't work. This report should not include your source code but should clarify your solution and any issues you faced.\n",
      "\n",
      "2. **<StudentID1_StudentID2>.patch**:  \n",
      "   - A git diff file showing the changes you made to the xv6 codebase. You can generate this by running the command:  \n",
      "     ```bash\n",
      "     $ git diff > <StudentID1_StudentID2>.patch\n",
      "     ```\n",
      "\n",
      "3. **Zip file of xv6**:  \n",
      "   - A zip file containing the modified xv6 codebase. The code should be in a clean state (i.e., after running `make clean`). The filename should follow the format:  \n",
      "     ```bash\n",
      "     <StudentID1_StudentID2>.zip\n",
      "     ```  \n",
      "     For example, if the students' IDs are 2312001 and 2312002, the filename would be:  \n",
      "     ```bash\n",
      "     2312001_2312002.zip\n",
      "     ```\n",
      "\n",
      "Make sure to follow these submission guidelines carefully to ensure your work is graded properly.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question related to the content of the PDF\n",
    "query = \"What submissions do I need to submit in this lab?\"\n",
    "print(\"Query: \", query)\n",
    "response = query_pdf(query, model_name=\"mlx-community/Qwen3-4B-8bit\")\n",
    "print(\"Response: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8eee4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  What is the hint for Lab 4.2 – Speed up system calls?\n",
      "Response:  The hint for Lab 4.2 – Speed up system calls is to choose permission bits that allow userspace to only read the page. This ensures that the shared page between userspace and the kernel can be accessed by userspace for reading the PID, but not modified, which is necessary for the optimization of the getpid() system call.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question related to the content of the PDF\n",
    "query = \"What is the hint for Lab 4.2 – Speed up system calls?\"\n",
    "print(\"Query: \", query)\n",
    "response = query_pdf(query, model_name=\"mlx-community/Qwen3-4B-8bit\")\n",
    "print(\"Response: \", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
