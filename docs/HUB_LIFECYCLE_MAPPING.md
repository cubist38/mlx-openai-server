# Hub Lifecycle Convergence Notes

This document captures plan-streamlineHubLifecycle Part A step 1 by mapping the overlapping lifecycle flows between the daemon-driven `HubSupervisor` and the single-model server bootstrap. It focuses on where both paths already share behavior and where shared utilities need to stabilize worker, registry, and idle-controller logic.

## Lifecycle Map

| Area | HubSupervisor path | Single-server path | Shared needs surfaced |
| --- | --- | --- | --- |
| Manager bootstrap & logging | `_build_manager_for_record()` prepares `LazyHandlerManager`, file logging, and metadata before `start_model()` wires registry IDs and workers ([app/hub/daemon.py](app/hub/daemon.py#L523-L1014)). | `create_lifespan()` instantiates `LazyHandlerManager`, attaches on-change hooks, and records metadata plus handlers in the registry ([app/server.py](app/server.py#L1061-L1269)). | Need a shared factory that standardizes manager creation, per-model log sinks, and initial registry payloads so daemon/server cannot drift on defaults (log level, queue sizing, model identifiers). |
| Registry synchronization | `_execute_load_operation()` and `_execute_unload_operation()` push handler + metadata changes, while `_start_worker()` / `_stop_worker()` update worker ports inside registry entries ([app/hub/daemon.py](app/hub/daemon.py#L574-L775)). | `_sync_registry_update()` inside `create_lifespan()` keeps registry metadata (VRAM timestamps, status, model path) authoritative ([app/server.py](app/server.py#L1165-L1219)). | Introduce a registry helper (Plan suggestion: `RegistrySyncService`) that encapsulates metadata merging, worker port management, and VRAM timestamps so both flows call the same code for handler-loaded, unloaded, and worker lifecycle events. |
| Worker lifecycle | `start_model()` bootstraps sidecar `SidecarWorker` instances and keeps `_start_worker()` idempotent; `_stop_worker()` clears worker ports and surfaces errors ([app/hub/daemon.py](app/hub/daemon.py#L574-L1045)). | Single-model server has no sidecar worker abstraction, but relies on handler residency states to expose `/v1/models` details ([app/server.py](app/server.py#L1061-L1344)). | Need a shared coordinator class (Plan callout: `SidecarWorkerCoordinator`) so daemon and any future server modules interact with workers via the same interface and propagate registry metadata consistently even when only one worker path exists today. |
| Idle controller & auto-unload | `HubSupervisor` accepts an injected `idle_controller`, uses it while building status snapshots, and enforces group eviction thresholds via `_ensure_group_capacity()` ([app/hub/daemon.py](app/hub/daemon.py#L705-L928) and [get_status() snapshot](app/hub/daemon.py#L1484-L1687)). | `CentralIdleAutoUnloadController` watches the registry, registers callbacks via the lifespan, and provides `get_expected_unload_timestamp()` for downstream reporting ([app/server.py](app/server.py#L827-L1054)). | Need a common idle/unload service so daemon status, CLI, and registry polling surface the same unload timestamps and eviction logic. Today duplicate timers lead to diverging `unload_timestamp` semantics between hub daemon snapshots and single-server metadata. |
| Status & snapshot assembly | `get_status()` composes `models` plus group payloads, probes workers, and injects idle controller timestamps ([app/hub/daemon.py](app/hub/daemon.py#L1484-L1775)). | Hub-aware server builds `/hub/status` payloads by combining config + registry snapshots inside `_build_models_from_config()` ([app/api/hub_routes.py](app/api/hub_routes.py#L238-L333)). | Need a shared formatter that converts supervisor/live registry data into the canonical snapshot, ensuring daemon UI, CLI, and hub routes reuse identical timestamp fields (e.g., `last_activity_ts`, `unload_timestamp`, group membership lists). |

## Cross-cutting Observations

- Both flows gate JIT load/unload decisions through `LazyHandlerManager`, but the daemon wraps it manually while the server wires it through FastAPI lifespan. Consolidating construction + teardown will eliminate branching logic around logging, registry hooks, and idle callbacks.
- Registry metadata schemas already overlap (worker ports, VRAM timestamps, group metadata), yet each code path populates fields differently. Centralizing the write paths will unblock Plan Part A step 2 without risking regressions in `/v1/models` responses.
- The daemon status builder currently probes workers for memory state, while the server trusts registry data. Sharing a formatter plus probe helper will let the server UI depend on the same load/unload timestamps that the daemon surfaces after `_probe_worker_for_memory()`.
- CLI, daemon UI, and API route handlers all need a unified `HubLifecycleService` wrapper that exposes `start/load/unload/stop/status` and internally orchestrates registry + worker helpers. This document identifies where that orchestration logic already exists so the next steps can safely relocate it under `app/core/`.

## Hub Service Shim Retirement Checklist

1. ✅ Removed unused shim endpoints in [app/api/hub_routes.py](app/api/hub_routes.py#L1-L600); dashboard buttons now call the canonical daemon routes described in Plan Part B/C.
2. ✅ Dropped shim-only schemas in [app/schemas/openai.py](app/schemas/openai.py) so only daemon payloads mirroring `HubSupervisor` snapshots remain.
3. ✅ Updated the dashboard template [templates/hub_status.html.jinja](templates/hub_status.html.jinja) so start/stop/load/unload buttons call `/hub/models/{name}/...` and all UI labels reference the daemon instead of the removed service shim.
4. ✅ Replaced `test_hub_service.py` with daemon-focused API coverage (`tests/test_hub_daemon_routes.py`) exercising `/hub/models/*`, `/hub/reload`, and `/hub/shutdown`.
5. ✅ Refreshed hub documentation ([docs/HUB_MODE.md](docs/HUB_MODE.md), [docs/HANDOFFS.md](docs/HANDOFFS.md), README sections) to describe the daemon-only control plane now that the shim is gone.
